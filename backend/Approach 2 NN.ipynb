{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question classification using LSTM networks and word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting random seed\n",
    "seed = 512\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3 vector files\n",
    "GLOVE_FILE = 'embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "PARAGRAM_FILE = 'embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "WIKI_NEWS_FILE = 'embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_coefficients(word, *arr): \n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "def get_no_lines(file_name): \n",
    "    return sum(1 for _ in open(file_name, encoding=\"utf8\", errors='ignore'))\n",
    "\n",
    "def load_vector(file_name): \n",
    "    return dict(get_coefficients(*o.split(\" \")) for o in tqdm(open(file_name, encoding=\"utf8\", errors='ignore'), total=get_no_lines(file_name)) if len(o) > 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the 2 csv files\n",
    "reddit_df = pd.read_csv('reddit.csv')\n",
    "reddit_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative, positive = np.bincount(reddit_df['target'])\n",
    "total = negative + positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Text preprocessing and Word vectorization\n",
    "Preprocessing words and converting words into vectors so that can be given as input to neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0965f4411c274994a316fe53b09bb7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a796108a5ff4e03abfd921150aecb87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1048575 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "tqdm().pandas()\n",
    "\n",
    "puncts = [\",\",\".\",'\"',\":\",\")\",\"(\",\"-\",\"!\",\"?\",\"|\",\";\",\"'\",\"$\",\"&\",\"/\",\"[\",\"]\",\">\",\"%\",\"=\",\"#\",\"*\",\"+\",\"\\\\\",\"•\",\"~\",\"@\",\"£\",\"·\",\"_\",\"{\",\"}\",\"©\",\"^\",\"®\",\"`\",\"<\",\"→\",\"°\",\"€\",\"™\",\"›\",\"♥\",\"←\",\"×\",\"§\",\"″\",\"′\",\"█\",\"…\",\"“\",\"★\",\"”\",\"–\",\"●\",\"►\",\"−\",\"¢\",\"¬\",\"░\",\"¡\",\"¶\",\"↑\",\"±\",\"¿\",\"▾\",\"═\",\"¦\",\"║\",\"―\",\"¥\",\"▓\",\"—\",\"‹\",\"─\",\"▒\",\"：\",\"⊕\",\"▼\",\"▪\",\"†\",\"■\",\"’\",\"▀\",\"¨\",\"▄\",\"♫\",\"☆\",\"¯\",\"♦\",\"¤\",\"▲\",\"¸\",\"⋅\",\"‘\",\"∞\",\"∙\",\"）\",\"↓\",\"、\",\"│\",\"（\",\"»\",\"，\",\"♪\",\"╩\",\"╚\",\"・\",\"╦\",\"╣\",\"╔\",\"╗\",\"▬\",\"❤\",\"≤\",\"‡\",\"√\",\"◄\",\"━\",\"⇒\",\"▶\",\"≥\",\"╝\",\"♡\",\"◊\",\"。\",\"✈\",\"≡\",\"☺\",\"✔\",\"↵\",\"≈\",\"✓\",\"♣\",\"☎\",\"℃\",\"◦\",\"└\",\"‟\",\"～\",\"！\",\"○\",\"◆\",\"№\",\"♠\",\"▌\",\"✿\",\"▸\",\"⁄\",\"□\",\"❖\",\"✦\",\"．\",\"÷\",\"｜\",\"┃\",\"／\",\"￥\",\"╠\",\"↩\",\"✭\",\"▐\",\"☼\",\"☻\",\"┐\",\"├\",\"«\",\"∼\",\"┌\",\"℉\",\"☮\",\"฿\",\"≦\",\"♬\",\"✧\",\"〉\",\"－\",\"⌂\",\"✖\",\"･\",\"◕\",\"※\",\"‖\",\"◀\",\"‰\",\"\\x97\",\"↺\",\"∆\",\"┘\",\"┬\",\"╬\",\"،\",\"⌘\",\"⊂\",\"＞\",\"〈\",\"⎙\",\"？\",\"☠\",\"⇐\",\"▫\",\"∗\",\"∈\",\"≠\",\"♀\",\"♔\",\"˚\",\"℗\",\"┗\",\"＊\",\"┼\",\"❀\",\"＆\",\"∩\",\"♂\",\"‿\",\"∑\",\"‣\",\"➜\",\"┛\",\"⇓\",\"☯\",\"⊖\",\"☀\",\"┳\",\"；\",\"∇\",\"⇑\",\"✰\",\"◇\",\"♯\",\"☞\",\"´\",\"↔\",\"┏\",\"｡\",\"◘\",\"∂\",\"✌\",\"♭\",\"┣\",\"┴\",\"┓\",\"✨\",\"\\xa0\",\"˜\",\"❥\",\"┫\",\"℠\",\"✒\",\"［\",\"∫\",\"\\x93\",\"≧\",\"］\",\"\\x94\",\"∀\",\"♛\",\"\\x96\",\"∨\",\"◎\",\"↻\",\"⇩\",\"＜\",\"≫\",\"✩\",\"✪\",\"♕\",\"؟\",\"₤\",\"☛\",\"╮\",\"␊\",\"＋\",\"┈\",\"％\",\"╋\",\"▽\",\"⇨\",\"┻\",\"⊗\",\"￡\",\"।\",\"▂\",\"✯\",\"▇\",\"＿\",\"➤\",\"✞\",\"＝\",\"▷\",\"△\",\"◙\",\"▅\",\"✝\",\"∧\",\"␉\",\"☭\",\"┊\",\"╯\",\"☾\",\"➔\",\"∴\",\"\\x92\",\"▃\",\"↳\",\"＾\",\"׳\",\"➢\",\"╭\",\"➡\",\"＠\",\"⊙\",\"☢\",\"˝\",\"∏\",\"„\",\"∥\",\"❝\",\"☐\",\"▆\",\"╱\",\"⋙\",\"๏\",\"☁\",\"⇔\",\"▔\",\"\\x91\",\"➚\",\"◡\",\"╰\",\"\\x85\",\"♢\",\"˙\",\"۞\",\"✘\",\"✮\",\"☑\",\"⋆\",\"ⓘ\",\"❒\",\"☣\",\"✉\",\"⌊\",\"➠\",\"∣\",\"❑\",\"◢\",\"ⓒ\",\"\\x80\",\"〒\",\"∕\",\"▮\",\"⦿\",\"✫\",\"✚\",\"⋯\",\"♩\",\"☂\",\"❞\",\"‗\",\"܂\",\"☜\",\"‾\",\"✜\",\"╲\",\"∘\",\"⟩\",\"＼\",\"⟨\",\"·\",\"✗\",\"♚\",\"∅\",\"ⓔ\",\"◣\",\"͡\",\"‛\",\"❦\",\"◠\",\"✄\",\"❄\",\"∃\",\"␣\",\"≪\",\"｢\",\"≅\",\"◯\",\"☽\",\"∎\",\"｣\",\"❧\",\"̅\",\"ⓐ\",\"↘\",\"⚓\",\"▣\",\"˘\",\"∪\",\"⇢\",\"✍\",\"⊥\",\"＃\",\"⎯\",\"↠\",\"۩\",\"☰\",\"◥\",\"⊆\",\"✽\",\"⚡\",\"↪\",\"❁\",\"☹\",\"◼\",\"☃\",\"◤\",\"❏\",\"ⓢ\",\"⊱\",\"➝\",\"̣\",\"✡\",\"∠\",\"｀\",\"▴\",\"┤\",\"∝\",\"♏\",\"ⓐ\",\"✎\",\";\",\"␤\",\"＇\",\"❣\",\"✂\",\"✤\",\"ⓞ\",\"☪\",\"✴\",\"⌒\",\"˛\",\"♒\",\"＄\",\"✶\",\"▻\",\"ⓔ\",\"◌\",\"◈\",\"❚\",\"❂\",\"￦\",\"◉\",\"╜\",\"̃\",\"✱\",\"╖\",\"❉\",\"ⓡ\",\"↗\",\"ⓣ\",\"♻\",\"➽\",\"׀\",\"✲\",\"✬\",\"☉\",\"▉\",\"≒\",\"☥\",\"⌐\",\"♨\",\"✕\",\"ⓝ\",\"⊰\",\"❘\",\"＂\",\"⇧\",\"̵\",\"➪\",\"▁\",\"▏\",\"⊃\",\"ⓛ\",\"‚\",\"♰\",\"́\",\"✏\",\"⏑\",\"̶\",\"ⓢ\",\"⩾\",\"￠\",\"❍\",\"≃\",\"⋰\",\"♋\",\"､\",\"̂\",\"❋\",\"✳\",\"ⓤ\",\"╤\",\"▕\",\"⌣\",\"✸\",\"℮\",\"⁺\",\"▨\",\"╨\",\"ⓥ\",\"♈\",\"❃\",\"☝\",\"✻\",\"⊇\",\"≻\",\"♘\",\"♞\",\"◂\",\"✟\",\"⌠\",\"✠\",\"☚\",\"✥\",\"❊\",\"ⓒ\",\"⌈\",\"❅\",\"ⓡ\",\"♧\",\"ⓞ\",\"▭\",\"❱\",\"ⓣ\",\"∟\",\"☕\",\"♺\",\"∵\",\"⍝\",\"ⓑ\",\"✵\",\"✣\",\"٭\",\"♆\",\"ⓘ\",\"∶\",\"⚜\",\"◞\",\"்\",\"✹\",\"➥\",\"↕\",\"̳\",\"∷\",\"✋\",\"➧\",\"∋\",\"̿\",\"ͧ\",\"┅\",\"⥤\",\"⬆\",\"⋱\",\"☄\",\"↖\",\"⋮\",\"۔\",\"♌\",\"ⓛ\",\"╕\",\"♓\",\"❯\",\"♍\",\"▋\",\"✺\",\"⭐\",\"✾\",\"♊\",\"➣\",\"▿\",\"ⓑ\",\"♉\",\"⏠\",\"◾\",\"▹\",\"⩽\",\"↦\",\"╥\",\"⍵\",\"⌋\",\"։\",\"➨\",\"∮\",\"⇥\",\"ⓗ\",\"ⓓ\",\"⁻\",\"⎝\",\"⌥\",\"⌉\",\"◔\",\"◑\",\"✼\",\"♎\",\"♐\",\"╪\",\"⊚\",\"☒\",\"⇤\",\"ⓜ\",\"⎠\",\"◐\",\"⚠\",\"╞\",\"◗\",\"⎕\",\"ⓨ\",\"☟\",\"ⓟ\",\"♟\",\"❈\",\"↬\",\"ⓓ\",\"◻\",\"♮\",\"❙\",\"♤\",\"∉\",\"؛\",\"⁂\",\"ⓝ\",\"־\",\"♑\",\"╫\",\"╓\",\"╳\",\"⬅\",\"☔\",\"☸\",\"┄\",\"╧\",\"׃\",\"⎢\",\"❆\",\"⋄\",\"⚫\",\"̏\",\"☏\",\"➞\",\"͂\",\"␙\",\"ⓤ\",\"◟\",\"̊\",\"⚐\",\"✙\",\"↙\",\"̾\",\"℘\",\"✷\",\"⍺\",\"❌\",\"⊢\",\"▵\",\"✅\",\"ⓖ\",\"☨\",\"▰\",\"╡\",\"ⓜ\",\"☤\",\"∽\",\"╘\",\"˹\",\"↨\",\"♙\",\"⬇\",\"♱\",\"⌡\",\"⠀\",\"╛\",\"❕\",\"┉\",\"ⓟ\",\"̀\",\"♖\",\"ⓚ\",\"┆\",\"⎜\",\"◜\",\"⚾\",\"⤴\",\"✇\",\"╟\",\"⎛\",\"☩\",\"➲\",\"➟\",\"ⓥ\",\"ⓗ\",\"⏝\",\"◃\",\"╢\",\"↯\",\"✆\",\"˃\",\"⍴\",\"❇\",\"⚽\",\"╒\",\"̸\",\"♜\",\"☓\",\"➳\",\"⇄\",\"☬\",\"⚑\",\"✐\",\"⌃\",\"◅\",\"▢\",\"❐\",\"∊\",\"☈\",\"॥\",\"⎮\",\"▩\",\"ு\",\"⊹\",\"‵\",\"␔\",\"☊\",\"➸\",\"̌\",\"☿\",\"⇉\",\"⊳\",\"╙\",\"ⓦ\",\"⇣\",\"｛\",\"̄\",\"↝\",\"⎟\",\"▍\",\"❗\",\"״\",\"΄\",\"▞\",\"◁\",\"⛄\",\"⇝\",\"⎪\",\"♁\",\"⇠\",\"☇\",\"✊\",\"ி\",\"｝\",\"⭕\",\"➘\",\"⁀\",\"☙\",\"❛\",\"❓\",\"⟲\",\"⇀\",\"≲\",\"ⓕ\",\"⎥\",\"\\u06dd\",\"ͤ\",\"₋\",\"̱\",\"̎\",\"♝\",\"≳\",\"▙\",\"➭\",\"܀\",\"ⓖ\",\"⇛\",\"▊\",\"⇗\",\"̷\",\"⇱\",\"℅\",\"ⓧ\",\"⚛\",\"̐\",\"̕\",\"⇌\",\"␀\",\"≌\",\"ⓦ\",\"⊤\",\"̓\",\"☦\",\"ⓕ\",\"▜\",\"➙\",\"ⓨ\",\"⌨\",\"◮\",\"☷\",\"◍\",\"ⓚ\",\"≔\",\"⏩\",\"⍳\",\"℞\",\"┋\",\"˻\",\"▚\",\"≺\",\"ْ\",\"▟\",\"➻\",\"̪\",\"⏪\",\"̉\",\"⎞\",\"┇\",\"⍟\",\"⇪\",\"▎\",\"⇦\",\"␝\",\"⤷\",\"≖\",\"⟶\",\"♗\",\"̴\",\"♄\",\"ͨ\",\"̈\",\"❜\",\"̡\",\"▛\",\"✁\",\"➩\",\"ா\",\"˂\",\"↥\",\"⏎\",\"⎷\",\"̲\",\"➖\",\"↲\",\"⩵\",\"̗\",\"❢\",\"≎\",\"⚔\",\"⇇\",\"̑\",\"⊿\",\"̖\",\"☍\",\"➹\",\"⥊\",\"⁁\",\"✢\"];\n",
    "contraction_dict = {\"We'd\": \"We had\", \"That'd\": \"That had\", \"AREN'T\": \"Are not\", \"HADN'T\": \"Had not\", \"Could've\": \"Could have\", \"LeT's\": \"Let us\", \"How'll\": \"How will\", \"They'll\": \"They will\", \"DOESN'T\": \"Does not\", \"HE'S\": \"He has\", \"O'Clock\": \"Of the clock\", \"Who'll\": \"Who will\", \"What'S\": \"What is\", \"Ain't\": \"Am not\", \"WEREN'T\": \"Were not\", \"Y'all\": \"You all\", \"Y'ALL\": \"You all\", \"Here's\": \"Here is\", \"It'd\": \"It had\", \"Should've\": \"Should have\", \"I'M\": \"I am\", \"ISN'T\": \"Is not\", \"Would've\": \"Would have\", \"He'll\": \"He will\", \"DON'T\": \"Do not\", \"She'd\": \"She had\", \"WOULDN'T\": \"Would not\", \"She'll\": \"She will\", \"IT's\": \"It is\", \"There'd\": \"There had\", \"It'll\": \"It will\", \"You'll\": \"You will\", \"He'd\": \"He had\", \"What'll\": \"What will\", \"Ma'am\": \"Madam\", \"CAN'T\": \"Can not\", \"THAT'S\": \"That is\", \"You've\": \"You have\", \"She's\": \"She is\", \"Weren't\": \"Were not\", \"They've\": \"They have\", \"Couldn't\": \"Could not\", \"When's\": \"When is\", \"Haven't\": \"Have not\", \"We'll\": \"We will\", \"That's\": \"That is\", \"We're\": \"We are\", \"They're\": \"They' are\", \"You'd\": \"You would\", \"How'd\": \"How did\", \"What're\": \"What are\", \"Hasn't\": \"Has not\", \"Wasn't\": \"Was not\", \"Won't\": \"Will not\", \"There's\": \"There is\", \"Didn't\": \"Did not\", \"Doesn't\": \"Does not\", \"You're\": \"You are\", \"He's\": \"He is\", \"SO's\": \"So is\", \"We've\": \"We have\", \"Who's\": \"Who is\", \"Wouldn't\": \"Would not\", \"Why's\": \"Why is\", \"WHO's\": \"Who is\", \"Let's\": \"Let us\", \"How's\": \"How is\", \"Can't\": \"Can not\", \"Where's\": \"Where is\", \"They'd\": \"They had\", \"Don't\": \"Do not\", \"Shouldn't\":\"Should not\", \"Aren't\":\"Are not\", \"ain't\": \"is not\", \"What's\": \"What is\", \"It's\": \"It is\", \"Isn't\":\"Is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "def add_tag(text):\n",
    "     #replacing sentences 'http' or 'www' with [url]\n",
    "    if 'http' in text or 'www' in text:\n",
    "        text = re.sub('(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+', '[url]', text)\n",
    "    \n",
    "    #replacing formulas with [formuala]\n",
    "    if '[math]' in text:\n",
    "        text = re.sub('\\[math\\].*?math\\]', '[formula]', text) \n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_contractions(text):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    \n",
    "    text = ' '.join([contraction_dict[t] if t in contraction_dict else t for t in text.split(\" \")])\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punct(x):\n",
    "    for punct in puncts:\n",
    "        if punct in x:\n",
    "            x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def data_cleaning(x):\n",
    "    x = add_tag(x)\n",
    "    x = remove_contractions(x)\n",
    "    x = remove_punct(x)\n",
    "    return x\n",
    "\n",
    "# doing preprocessing on reddit data\n",
    "reddit_df['preprocessed_question_text'] = reddit_df['question_text'].progress_map(lambda x: data_cleaning(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 210084 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# creating dict for vocab_freq, word2index and lemma_dict\n",
    "vocab_freq = {}\n",
    "word2index = {}\n",
    "lemma_dict = {}\n",
    "\n",
    "sentences = reddit_df[\"preprocessed_question_text\"]\n",
    "word_sequences = []\n",
    "\n",
    "for doc in tqdm(sentences):\n",
    "    word_seq = []\n",
    "    for token in nlp(doc):\n",
    "        if token.is_punct or token.is_space:\n",
    "            continue\n",
    "        try:\n",
    "            vocab_freq[token.text] += 1\n",
    "        except KeyError:\n",
    "            vocab_freq[token.text] = 1\n",
    "        if token.text not in word2index:\n",
    "            word2index[token.text] = len(vocab_freq)\n",
    "            lemma_dict[token.text] = token.lemma_\n",
    "        word_seq.append(word2index[token.text])\n",
    "    word_sequences.append(word_seq)\n",
    "\n",
    "vocab_size = len(word2index)\n",
    "\n",
    "print('Found %s unique tokens.' % len(word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max question length in data:  145\n",
      "Data tensor shape: (1048575, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SENTENCE_LENGTH = 100\n",
    "\n",
    "max_que_len = len(max(word_sequences, key=len))\n",
    "print(\"Max question length in data: \", max_que_len)\n",
    "\n",
    "X_train = word_sequences[:len(reddit_df)]\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=MAX_SENTENCE_LENGTH)\n",
    "print('Data tensor shape:', X_train.shape)\n",
    "\n",
    "X_test_data = word_sequences[len(reddit_df):]\n",
    "X_test_data = keras.preprocessing.sequence.pad_sequences(X_test_data, maxlen=MAX_SENTENCE_LENGTH)\n",
    "\n",
    "y_train = reddit_df['target']\n",
    "\n",
    "del reddit_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, SnowballStemmer, WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    " \n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lc = LancasterStemmer()\n",
    "sb = SnowballStemmer('english')\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "def correction(word): return list((known([word]) or known(edits1(word)) or [word]))[0]\n",
    "\n",
    "def known(words): return set(w for w in words if w in word2index)\n",
    "\n",
    "def edits1(word):\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:])        for i in range(len(word) + 1)]\n",
    "    deletes = [L + R[1:]                  for L, R in splits if R]  \n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]  \n",
    "    replaces = [L + c + R[1:]             for L, R in splits if R for c in letters]\n",
    "    inserts = [L + c + R                  for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def load_embedding(word2vec):\n",
    "    oov_count = 0\n",
    "    vocab_count = 0\n",
    "    embedding_weights = np.zeros((vocab_size+1, EMBEDDING_DIM))\n",
    "    unknown_vector = np.zeros((EMBEDDING_DIM,), dtype=np.float32) - 1.\n",
    "    unknown_words = {}\n",
    "\n",
    "    for key, i in tqdm(word2index.items()):\n",
    "        word = key\n",
    "        if word in word2vec:\n",
    "            vocab_count += vocab_freq[key]\n",
    "            embedding_weights[i] = word2vec[word]\n",
    "            continue\n",
    "        #Lower\n",
    "        word = key.lower()         \n",
    "        if word in word2vec:\n",
    "            vocab_count += vocab_freq[key]\n",
    "            embedding_weights[i] = word2vec[word]\n",
    "            continue\n",
    "        #Upper\n",
    "        word = key.upper()         \n",
    "        if word in word2vec:\n",
    "            vocab_count += vocab_freq[key]\n",
    "            embedding_weights[i] = word2vec[word]\n",
    "            continue\n",
    "        #Capitalize\n",
    "        word = key.capitalize()     \n",
    "        if word in word2vec:\n",
    "            vocab_count += vocab_freq[key]\n",
    "            embedding_weights[i-1] = word2vec[word]\n",
    "            continue\n",
    "        #PorterStemmer\n",
    "        word = ps.stem(key)        \n",
    "        if word in word2vec:\n",
    "            vocab_count += vocab_freq[key]\n",
    "            embedding_weights[i] = word2vec[word]\n",
    "            continue\n",
    "        #LancasterStemmer\n",
    "        word = lc.stem(key)        \n",
    "        if word in word2vec:\n",
    "            vocab_count += vocab_freq[key]\n",
    "            embedding_weights[i] = word2vec[word]\n",
    "            continue\n",
    "        #SnowballStemmer\n",
    "        word = sb.stem(key)        \n",
    "        if word in word2vec:\n",
    "            vocab_count += vocab_freq[key]\n",
    "            embedding_weights[i] = word2vec[word]\n",
    "            continue\n",
    "        #Lemmanization\n",
    "        word = lemma_dict[key]     \n",
    "        if word in word2vec: \n",
    "            vocab_count += vocab_freq[key]\n",
    "            embedding_weights[i] = word2vec[word]\n",
    "            continue\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            if word in word2vec: \n",
    "                vocab_count += vocab_freq[key]\n",
    "                embedding_weights[i] = word2vec[word]\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            unknown_words[key] += 1\n",
    "        except KeyError:\n",
    "            unknown_words[key] = 1\n",
    "            \n",
    "        embedding_weights[i] = unknown_vector\n",
    "        oov_count += vocab_freq[key]\n",
    "\n",
    "    print('Top 5 Null word embeddings: ')\n",
    "    print(list(unknown_words.items())[:5])\n",
    "    print('\\n')\n",
    "    print('Word embeddings which are null: %d' % np.sum(np.sum(embedding_weights, axis=1) == -1 * EMBEDDING_DIM))\n",
    "    print('Null word embeddings percentage: %.2f%%' % (100 * oov_count / vocab_count))\n",
    "    \n",
    "    return embedding_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading paragram_vec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25df2613ce264a6492668247fc750d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1703756 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b185d78a99454b8496c04444a6b1306f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210084 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Null word embeddings: \n",
      "[('calead', 1), ('nanodegree', 1), ('4AFSB', 1), ('Anizara', 1), ('Redmi', 1)]\n",
      "\n",
      "\n",
      "Word embeddings which are null: 33587\n",
      "Null word embeddings percentage: 0.33%\n",
      "loading glove_vec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e452d0350424c06abcbc21a53d8a5cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2196017 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1998621036d54d008190125ccd84cd45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210084 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Null word embeddings: \n",
      "[('calead', 1), ('Tepelene', 1), ('nanodegree', 1), ('4AFSB', 1), ('Anizara', 1)]\n",
      "\n",
      "\n",
      "Word embeddings which are null: 34780\n",
      "Null word embeddings percentage: 0.34%\n",
      "loading fasttext_vec\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f0638bac7204cceb6b8994e37101184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/999995 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "488829dde42e4631a32b7ad7d7b004f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/210084 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Null word embeddings: \n",
      "[('montra', 1), ('Mcleodganj', 1), ('calead', 1), ('isovolumetric', 1), ('nanodegree', 1)]\n",
      "\n",
      "\n",
      "Word embeddings which are null: 44706\n",
      "Null word embeddings percentage: 0.45%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('loading paragram_vec')\n",
    "paragram_vec = load_vector(PARAGRAM_FILE)\n",
    "paragram_weights = load_embedding(paragram_vec)\n",
    "del paragram_vec\n",
    "gc.collect()\n",
    "\n",
    "print('loading glove_vec')\n",
    "glove_vec = load_vector(GLOVE_FILE)\n",
    "glove_weights = load_embedding(glove_vec)\n",
    "del glove_vec\n",
    "gc.collect()\n",
    "\n",
    "print('loading fasttext_vec')\n",
    "fasttext_vec = load_vector(WIKI_NEWS_FILE) \n",
    "fasttext_weights = load_embedding(fasttext_vec)\n",
    "del fasttext_vec\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.  Model Training and Evaluation  \n",
    "- There has 3 layers in the model. \n",
    "- The 1st layer is Embedding layer that turns the X_train data(now that's the word indexes of vocabulary) into EMBEDDING_DIM dimensional vectors. \n",
    "- The 2nd layer is a bidirectinal LSTM that is well-suited to process data base on time-series. \n",
    "- The 3rd layer is output layer with a sigmoid activation function. \n",
    "- We had set the bias_initializer parameter for the output layer due to the imbalance in the dataset.\n",
    "- We used metrics f1-score for binary classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from keras import backend as K\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.initializers import Constant\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "def build_model(units):\n",
    "    output_bias = Constant(np.log([positive/negative]))\n",
    "    \n",
    "    x_in = Input(shape=(MAX_SENTENCE_LENGTH,))\n",
    "    glove_embedding = Embedding(len(glove_weights), EMBEDDING_DIM, input_length=MAX_SENTENCE_LENGTH, weights=[glove_weights], trainable=False)(x_in)\n",
    "#     paragram_embedding = Embedding(len(paragram_weights), EMBEDDING_DIM, input_length=MAX_SENTENCE_LENGTH, weights=[paragram_weights], trainable=False)(x_in)\n",
    "#     fasttext_embedding = Embedding(len(fasttext_weights), EMBEDDING_DIM, input_length=MAX_SENTENCE_LENGTH, weights=[fasttext_weights], trainable=False)(x_in)\n",
    "#     x = Concatenate()([glove_embedding, paragram_embedding, fasttext_embedding])\n",
    "    \n",
    "    x = SpatialDropout1D(0.2)(glove_embedding)\n",
    "    lstm = Bidirectional(tf.keras.layers.LSTM(units, return_sequences=True))(x)\n",
    "    gru = Bidirectional(GRU(units, return_sequences=True))(lstm)\n",
    "    \n",
    "    x = Concatenate()([lstm, gru])\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x_out = Dense(1, activation='sigmoid', bias_initializer=output_bias)(x)\n",
    "    \n",
    "    model = Model(inputs=x_in, outputs=x_out)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])   \n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.title('model loss')\n",
    "    plt.legend(['train', 'validation'])\n",
    "    plt.show()\n",
    "\n",
    "def f1_eval(y_true, y_pred):\n",
    "    args = np.argsort(y_pred)\n",
    "    tp = y_true.sum()\n",
    "    fs = (tp - np.cumsum(y_true[args[:-1]])) / np.arange(y_true.shape[0] + tp - 1, tp, -1)\n",
    "    res_idx = np.argmax(fs)\n",
    "    return 2 * fs[res_idx], (y_pred[args[res_idx]] + y_pred[args[res_idx + 1]]) / 2\n",
    "\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1)\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', verbose=1, patience=1, mode='min', restore_best_weights=True)\n",
    "\n",
    "checkpoint = keras.callbacks.ModelCheckpoint('lg_model.h5', monitor='val_loss', save_best_only=True, mode='min')\n",
    "\n",
    "weight_for_0 = (1 / negative) * (total) / 2.0 \n",
    "weight_for_1 = (1 / positive) * (total) / 2.0\n",
    "\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "strategy = None\n",
    "\n",
    "try:\n",
    "    if len(tf.config.list_physical_devices('GPU')) > 0:\n",
    "        strategy = tf.distribute.MirroredStrategy()\n",
    "        print('Using GPU') \n",
    "    else:\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "        print('Using CPU')\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print('Using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 300)     63025500    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d (SpatialDropo (None, 100, 300)     0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 100, 128)     186880      spatial_dropout1d[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 100, 128)     74496       bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 100, 256)     0           bidirectional[0][0]              \n",
      "                                                                 bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 256)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            257         global_average_pooling1d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 63,287,133\n",
      "Trainable params: 261,633\n",
      "Non-trainable params: 63,025,500\n",
      "__________________________________________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iadit\\anaconda3\\envs\\dm\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:3503: UserWarning: Even though the tf.config.experimental_run_functions_eagerly option is set, this option does not apply to tf.data functions. tf.data functions are still traced and executed as graphs.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6554/6554 [==============================] - 641s 95ms/step - loss: 0.3258 - val_loss: 0.2565\n",
      "Epoch 2/5\n",
      "6554/6554 [==============================] - 822s 125ms/step - loss: 0.2475 - val_loss: 0.2632\n",
      "Epoch 3/5\n",
      "6554/6554 [==============================] - 431s 66ms/step - loss: 0.2214 - val_loss: 0.2153\n",
      "Epoch 4/5\n",
      "6554/6554 [==============================] - 432s 66ms/step - loss: 0.2072 - val_loss: 0.2500\n",
      "Epoch 5/5\n",
      "6554/6554 [==============================] - 411s 63ms/step - loss: 0.1911 - val_loss: 0.2119\n",
      "Optimal F1: 0.6789 at threshold: 0.8921\n",
      "\n",
      "Epoch 1/5\n",
      "6554/6554 [==============================] - 365s 56ms/step - loss: 0.1987 - val_loss: 0.2363\n",
      "Epoch 2/5\n",
      "6554/6554 [==============================] - 402s 61ms/step - loss: 0.1920 - val_loss: 0.2259\n",
      "Epoch 3/5\n",
      "6554/6554 [==============================] - 380s 58ms/step - loss: 0.1852 - val_loss: 0.2100\n",
      "Epoch 4/5\n",
      "6554/6554 [==============================] - 380s 58ms/step - loss: 0.1795 - val_loss: 0.2091\n",
      "Epoch 5/5\n",
      "6554/6554 [==============================] - 405s 62ms/step - loss: 0.1734 - val_loss: 0.1970\n",
      "Optimal F1: 0.7067 at threshold: 0.8920\n",
      "\n",
      "Best f1-score:  0.70666066294898\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from IPython.display import Image\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=seed)\n",
    "\n",
    "best_f1score = 0\n",
    "\n",
    "with strategy.scope():\n",
    "    model = build_model(64)\n",
    "    model.summary()\n",
    "    \n",
    "    plot_model(model, 'lg_model.png', show_shapes=True, show_layer_names=True)\n",
    "    \n",
    "    for index, (train_index, valid_index) in enumerate(kfold.split(X_train, y_train)):\n",
    "        if index > 1:\n",
    "            break\n",
    "        X_trainn, X_val, Y_trainn, Y_val = X_train[train_index], X_train[valid_index], y_train[train_index], y_train[valid_index]\n",
    "        history = model.fit(X_trainn, Y_trainn, epochs=5, batch_size=128, validation_data=(X_val, Y_val), callbacks=[reduce_lr, checkpoint], class_weight=class_weight)\n",
    "        Y_pred = model.predict(X_val)\n",
    "        f1, threshold = f1_eval(Y_val.to_numpy(), np.squeeze(Y_pred))\n",
    "        best_f1score = max(best_f1score, f1)\n",
    "        print('Optimal F1: {:.4f} at threshold: {:.4f}\\n'.format(f1, threshold))\n",
    "\n",
    "print('Best f1-score: ', best_f1score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
